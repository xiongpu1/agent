import os
import sys
import json
import subprocess
import tempfile
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from difflib import SequenceMatcher

import pandas as pd
try:
    import litellm  # type: ignore
    from src.models_litellm import *  # type: ignore  # noqa: F403
except Exception:  # noqa: BLE001
    litellm = None

# Ensure `backend/` is on sys.path so imports like `src.*` work when running
# `python backend/src/extract_node_name_from_xlsx.py` from repo root.
_BACKEND_DIR = Path(__file__).resolve().parent.parent
if str(_BACKEND_DIR) not in sys.path:
    sys.path.insert(0, str(_BACKEND_DIR))

from src.rag_bom import decode_bom_code

# --- ç³»ç»Ÿæç¤ºæ¨¡æ¿ ---
SYSTEM_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„ Python æ•°æ®åˆ†æä»£ç†ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€ä¸ª Excel æ–‡ä»¶ï¼Œå¹¶æå–äº§å“åŠå…¶é…ä»¶åˆ—è¡¨ã€‚

ä½ çš„è§„åˆ™ï¼š
1.  ä½ åªèƒ½å“åº”çº¯ç²¹çš„ Python ä»£ç ã€‚ä¸è¦ä½¿ç”¨ markdown (ä¾‹å¦‚ ```python ... ```) æˆ–ä»»ä½•è§£é‡Šæ€§æ–‡æœ¬ã€‚
2.  ä½ çš„ä»£ç å°†åœ¨æœ¬æœºé€šè¿‡ `uv run` ä»¥éš”ç¦»çš„æ²™ç›’ Python ç¯å¢ƒæ‰§è¡Œï¼›è¯¥ç¯å¢ƒå¯ç”¨ä¸”ä»…æä¾› `pandas` ä¸ `openpyxl`ã€‚
3.  ä½ å¿…é¡»ä½¿ç”¨å·²æä¾›çš„å˜é‡ï¼š
    - `file_path`ï¼šç›®æ ‡ Excel æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰
    - `output_file_path`ï¼šæœ€ç»ˆ JSON è¾“å‡ºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰
4.  è¯¥ excel ä¸­ï¼š
    - äº§å“åçš„è‹±æ–‡å¯¹åº”äº"ç‰©æ–™ç¼–ç "åˆ—ï¼Œéœ€è¦å»æ‰å­—æ¯å‰çš„æ•°å­—ä»¥åŠç‰¹æ®Šç¬¦å·ï¼›
    - ä¸­æ–‡åç§°å¯¹åº”äº"ç‰©æ–™åç§°"åˆ—ï¼Œåªä¿ç•™ä¸­æ–‡å†…å®¹ï¼›
    - äº§å“çš„æ„æˆ/é…ä»¶/ç»„ä»¶å¯¹åº”äº"äº§å“é…ç½®"åˆ—ï¼Œå…¶ä¸­å„éƒ¨ä»¶åç§°ä»¥ä¸­æ–‡é€—å·æˆ–è‹±æ–‡é€—å·åˆ†éš”ï¼›
    - æ¯ä¸€ä¸ªäº§å“éƒ½æœ‰è‡ªå·±çš„ BOM ç‰ˆæœ¬ï¼Œå¯¹åº”äº"BOMç‰ˆæœ¬"åˆ—ã€‚
5.  è¯¥ excel ä¸­ä¼šå‡ºç°é‡å¤çš„äº§å“åä»¥åŠäº§å“é…ç½®ï¼Œéœ€è¦æ•°æ®é¢„å¤„ç†ï¼Œä½¿ç”¨ `drop_duplicates()` å»é‡ã€‚
6.  ä½ çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ª JSON æ–‡ä»¶ï¼Œæ ¼å¼ä¸ºï¼š
    [
      {"product_english_name": "äº§å“è‹±æ–‡åç§°", "product_chinese_name": "äº§å“ä¸­æ–‡åç§°", "bom_version": "BOMç‰ˆæœ¬", "accessories": ["æ„æˆ/é…ä»¶/ç»„ä»¶1", "æ„æˆ/é…ä»¶/ç»„ä»¶2", ...]},
      ...
    ]
    ä¸€ä¸ªäº§å“å¯¹åº”ä¸€å¥—äº§å“æ„æˆ/é…ä»¶/ç»„ä»¶ï¼Œç†è®ºä¸Šå°±æ˜¯æŸä¸€è¡Œçš„å†…å®¹ã€‚
7.  å¦‚æœ"äº§å“é…ç½®"æ²¡æœ‰æœ‰ç”¨çš„ä¿¡æ¯ï¼Œåˆ™ä¸ä¿å­˜è¯¥äº§å“ã€‚
8.  å·¥ä½œæµç¨‹ï¼š
    a. é¦–å…ˆï¼Œç¼–å†™ä»£ç æ¥åŠ è½½å¹¶æ£€æŸ¥æ–‡ä»¶ï¼ˆä¾‹å¦‚ `pd.read_excel(...)`, `df.head()`, `df.columns`ï¼‰ã€‚
    b. æ•°æ®é¢„å¤„ç†ï¼Œå»æ‰é‡å¤çš„å†…å®¹ã€‚
    c. æ ¹æ®äº§å“åç§°ã€BOM ç‰ˆæœ¬å’Œäº§å“é…ç½®ï¼Œç”Ÿæˆäº§å“åŠå…¶é…ä»¶åˆ—è¡¨ã€‚
    d. è¾“å‡ºäº§å“åŠå…¶é…ä»¶åˆ—è¡¨ï¼ˆæ‰“å°å¿…è¦çš„ä¸­é—´ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•ï¼‰ã€‚
    e. æ ¹æ®è¾“å‡ºè¿­ä»£å®Œå–„ä»£ç ã€‚
    f. æœ€åï¼Œç¼–å†™ä»£ç å°†è¯¥åˆ—è¡¨ä¿å­˜ä¸º `output_file_path`ã€‚
9.  è¾“å‡ºçš„ JSON å†…å®¹åº”å¿ å®äº excel åŸå§‹å†…å®¹ï¼Œä¸è¦è¿›è¡Œä»»ä½•ä¿®æ”¹/ç¾åŒ–/ç¿»è¯‘ç­‰æ”¹å˜äº‹å®æ•°æ®çš„æ“ä½œã€‚
10.  ä»»åŠ¡å®Œæˆä¿¡å·ï¼šå½“ä½ æˆåŠŸä¿å­˜ JSON æ–‡ä»¶åï¼Œä½ çš„æœ€åä¸€æ®µä»£ç å¿…é¡»æ‰“å° "TASK_COMPLETE"ã€‚è¿™æ˜¯æˆ‘åœæ­¢å¾ªç¯çš„å”¯ä¸€ä¿¡å·ã€‚
"""


def extract_node_name_from_xlsx(file_path: str, output_file_path: Optional[str] = None) -> list | dict:
    """
    ä» Excel æ–‡ä»¶ä¸­æå–äº§å“åŠå…¶é…ä»¶åˆ—è¡¨ï¼Œå¹¶ç”Ÿæˆ JSON æ–‡ä»¶ã€‚
    
    Args:
        file_path: Excel æ–‡ä»¶çš„è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰
    
    Returns:
        JSON æ ¼å¼çš„å†…å®¹ï¼ˆåˆ—è¡¨æˆ–å­—å…¸ï¼‰
    
    Raises:
        SystemExit: å¦‚æœæ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨
        FileNotFoundError: å¦‚æœç”Ÿæˆçš„ JSON æ–‡ä»¶ä¸å­˜åœ¨
        json.JSONDecodeError: å¦‚æœ JSON æ–‡ä»¶æ ¼å¼é”™è¯¯
    """
    # 1) éªŒè¯ç”¨æˆ·æä¾›çš„ file_path
    if not file_path:
        print("é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„ file_path å‚æ•°ã€‚")
        sys.exit(1)

    input_file_path = os.path.abspath(file_path)
    if not os.path.exists(input_file_path):
        # Common pitfall: calling from repo root, but file lives under backend/.
        # If a relative path was provided, try resolving it relative to backend/.
        try:
            if not os.path.isabs(file_path):
                backend_dir = Path(__file__).resolve().parent.parent
                alt = (backend_dir / file_path).resolve()
                if alt.exists():
                    input_file_path = str(alt)
        except Exception:
            pass
    if not os.path.exists(input_file_path):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° '{input_file_path}'ã€‚")
        sys.exit(1)

    # 2) è®¡ç®—è¾“å‡ºæ–‡ä»¶è·¯å¾„
    if output_file_path and str(output_file_path).strip():
        host_output_path = os.path.abspath(str(output_file_path).strip())
    else:
        xlsx_filename = os.path.basename(input_file_path)
        output_filename = os.path.splitext(xlsx_filename)[0] + "_accessories.json"
        host_output_path = os.path.join(os.path.dirname(input_file_path), output_filename)

    print("--- ä»»åŠ¡é…ç½® ---")
    print(f"ç›®æ ‡æ–‡ä»¶: {input_file_path}")
    print(f"ä¸»æœºä¸Šè¾“å‡º: {host_output_path}")
    print("------------------")

    code_to_product_type_zh: Dict[str, str] = {
        "31": "æ³³æ± ",
        "32": "æˆ·å¤–ç¼¸",
        "33": "æŒ‰æ‘©æµ´ç¼¸",
        "34": "å¯¹æ¥æŒ‰æ‘©ç¼¸",
        "35": "æŒ‰æ‘©ç¼¸",
        "37": "å†°æ°´ç¼¸",
    }

    translate_xlsx_path = (Path(__file__).resolve().parent.parent / "data_test" / "translate_accessories.xlsx").resolve()

    def _extract_prefix_and_material_code(raw_value: Any) -> Tuple[Optional[str], str]:
        s = str(raw_value or "").strip()
        if not s:
            return None, ""
        m = re.match(r"^\s*(\d+)\s*[\.ï¼]\s*(.+?)\s*$", s)
        if m:
            prefix = m.group(1)
            tail = m.group(2)
        else:
            m2 = re.match(r"^\s*(\d+)\s*(.+?)\s*$", s)
            if m2:
                prefix = m2.group(1)
                tail = m2.group(2)
            else:
                prefix = None
                tail = s

        tail = tail.strip()
        tail = re.sub(r"[^A-Za-z0-9]+", "", tail)
        return prefix, tail

    def _normalize_code_tail(raw_value: Any) -> str:
        _, tail = _extract_prefix_and_material_code(raw_value)
        return tail

    def _normalize_zh_text(raw_value: Any) -> str:
        s = str(raw_value or "").strip()
        if not s:
            return ""
        # keep Chinese + common symbols used in accessory names, remove whitespace
        s = re.sub(r"\s+", "", s)
        s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        return s

    def _is_na_like(s: str) -> bool:
        v = (s or "").strip()
        if not v:
            return True
        return v.lower() in {"#n/a", "n/a", "na", "nan", "none"}

    def _pick_english_name(name_en: Any, detail_en: Any) -> str:
        a = str(name_en or "").strip()
        b = str(detail_en or "").strip()
        if not _is_na_like(a):
            return a
        if not _is_na_like(b):
            return b
        return ""

    def _best_fuzzy_match(query: str, candidates: List[str], threshold: float) -> Optional[str]:
        if not query or not candidates:
            return None
        best_key = None
        best_score = 0.0
        for cand in candidates:
            score = SequenceMatcher(None, query, cand).ratio()
            if score > best_score:
                best_score = score
                best_key = cand
        if best_key is not None and best_score >= threshold:
            return best_key
        return None

    def _load_translate_table(path: Path) -> Optional[pd.DataFrame]:
        if not path.exists():
            return None
        try:
            return pd.read_excel(str(path), engine="openpyxl")
        except Exception:
            return None

    def _find_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
        for c in candidates:
            if c in df.columns:
                return c
        # fallback: substring contains
        for col in df.columns:
            col_s = str(col)
            for c in candidates:
                if c in col_s:
                    return col
        return None

    def _build_translation_index(df: pd.DataFrame) -> Tuple[Dict[Tuple[str, str], str], Dict[str, str]]:
        # Try to be robust to column naming.
        code_col = _find_col(df, ["ç¼–ç ", "ç‰©æ–™ç¼–ç ", "äº§å“ç¼–ç ", "å‹å·", "Code"])
        zh_col = _find_col(df, ["åç§°", "ä¸­æ–‡åç§°", "é…ä»¶åç§°", "ä¸­æ–‡", "Name"])
        en_col = _find_col(df, ["è‹±æ–‡åç§°", "è‹±æ–‡å", "EN", "English Name"])
        en_detail_col = _find_col(df, ["è‹±æ–‡è¯¦ç»†è¯´æ˜", "è‹±æ–‡è¯´æ˜", "è‹±æ–‡æè¿°", "English Description", "English Detail"])

        if zh_col is None:
            raise RuntimeError(f"ç¿»è¯‘è¡¨ç¼ºå°‘ä¸­æ–‡åç§°åˆ—ã€‚å®é™…åˆ—: {list(df.columns)}")

        by_code_and_zh: Dict[Tuple[str, str], str] = {}
        by_zh: Dict[str, str] = {}

        for _, r in df.iterrows():
            code_tail = _normalize_code_tail(r.get(code_col)) if code_col else ""
            zh = _normalize_zh_text(r.get(zh_col))
            if not zh:
                continue
            en = _pick_english_name(r.get(en_col) if en_col else None, r.get(en_detail_col) if en_detail_col else None)
            if not en:
                continue

            if code_tail:
                k = (code_tail, zh)
                if k not in by_code_and_zh:
                    by_code_and_zh[k] = en
            if zh not in by_zh:
                by_zh[zh] = en

        return by_code_and_zh, by_zh

    def _extract_zh_only(raw_value: Any) -> str:
        s = str(raw_value or "").strip()
        if not s:
            return ""
        parts = re.findall(r"[\u4e00-\u9fff]+", s)
        return "".join(parts).strip()

    def _split_accessories(config_text: str) -> List[str]:
        raw = (config_text or "").strip()
        if not raw:
            return []
        if raw.lower() == "nan":
            return []
        pieces = re.split(r"[ï¼Œ,;ï¼›\n]+", raw)
        out: List[str] = []
        seen = set()
        for p in pieces:
            s = (p or "").strip()
            if not s:
                continue
            if s.lower() == "nan":
                continue
            if s in seen:
                continue
            seen.add(s)
            out.append(s)
        return out

    def _bom_type_from_product_type_zh(product_type_zh: str) -> Optional[str]:
        s = (product_type_zh or "").strip()
        if s == "æ³³æ± ":
            return "pool"
        if s == "æˆ·å¤–ç¼¸":
            return "outdoor"
        if s == "å†°æ°´ç¼¸":
            return "iceTub"
        return None

    def _accessories_from_bom(bom_id: str, bom_type: str) -> List[str]:
        decoded = decode_bom_code(bom_id, bom_type=bom_type)
        if not decoded:
            return []
        segments = decoded.get("segments") or []
        out: List[str] = []
        seen = set()
        for seg in segments:
            meaning = seg.get("meaning") if isinstance(seg, dict) else None
            if not meaning or not isinstance(meaning, str):
                continue
            s = meaning.strip()
            if not s:
                continue
            # Filter out invalid placeholder meanings.
            if "æ ‡æ³¨é”™è¯¯" in s:
                continue
            if s in seen:
                continue
            seen.add(s)
            out.append(s)
        return out

    try:
        # Explicitly check openpyxl to provide a clearer error message.
        import openpyxl  # noqa: F401
    except ModuleNotFoundError as e:
        raise RuntimeError(
            "è¯»å– Excel å¤±è´¥ï¼šç¼ºå°‘ä¾èµ– openpyxlã€‚\n"
            "è¯·å®‰è£…åå†è¿è¡Œï¼š\n"
            "- conda install -n syp openpyxl\n"
            "æˆ–\n"
            "- pip install openpyxl\n"
            "æˆ–ä½¿ç”¨ uvï¼š\n"
            "- uv run --no-project --with pandas --with openpyxl python backend/src/extract_node_name_from_xlsx.py"
        ) from e

    try:
        df = pd.read_excel(input_file_path, engine="openpyxl")
    except Exception as e:
        raise RuntimeError(f"è¯»å– Excel å¤±è´¥: {e}") from e

    required_cols = ["ç‰©æ–™ç¼–ç ", "ç‰©æ–™åç§°", "äº§å“é…ç½®", "BOMç‰ˆæœ¬"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise RuntimeError(f"Excel ç¼ºå°‘åˆ—: {missing}. å®é™…åˆ—: {list(df.columns)}")

    items: List[Dict[str, Any]] = []
    dedupe = set()
    for _, row in df.iterrows():
        prefix, material_code = _extract_prefix_and_material_code(row.get("ç‰©æ–™ç¼–ç "))
        if not material_code:
            continue

        bom_id = str(row.get("BOMç‰ˆæœ¬") or "").strip()
        if not bom_id:
            continue

        name_zh = _extract_zh_only(row.get("ç‰©æ–™åç§°"))
        config_raw = row.get("äº§å“é…ç½®")
        if pd.isna(config_raw):
            continue
        config_text_zh = str(config_raw).strip()
        if not config_text_zh or config_text_zh.lower() == "nan":
            continue

        product_type_zh = code_to_product_type_zh.get(str(prefix or "").strip(), "æœªçŸ¥")
        bom_type = _bom_type_from_product_type_zh(product_type_zh)
        if not bom_type:
            continue

        accessories = _accessories_from_bom(bom_id, bom_type=bom_type)
        if not accessories:
            continue

        record = {
            "material_code": material_code,
            "bom_id": bom_id,
            "name_zh": name_zh,
            "name_en": material_code,
            "product_type_zh": product_type_zh,
            "config_text_zh": config_text_zh,
            "accessories": accessories,
        }

        key = (record["material_code"], record["bom_id"], record["config_text_zh"])
        if key in dedupe:
            continue
        dedupe.add(key)
        items.append(record)

    try:
        # Enrich with English accessory names (optional; only if translation table exists).
        translate_df = _load_translate_table(translate_xlsx_path)
        if translate_df is not None and not translate_df.empty:
            by_code_and_zh, by_zh = _build_translation_index(translate_df)

            total = 0
            hit_exact = 0
            hit_fuzzy_code = 0
            hit_exact_global = 0
            miss = 0

            zh_keys_by_code: Dict[str, List[str]] = {}
            for (code_tail, zh_key) in by_code_and_zh.keys():
                zh_keys_by_code.setdefault(code_tail, []).append(zh_key)

            # NOTE: Global fuzzy matching over all keys is very expensive (N accessories * M keys).
            # We only do global exact matching as a safe/fast fallback.

            # Memoize per (code_tail, zh_norm) to avoid repeated matching.
            cache: Dict[Tuple[str, str], str] = {}

            for item in items:
                code_tail = str(item.get("material_code") or "").strip()
                accs = item.get("accessories") or []
                accs_en: List[str] = []
                for acc in accs:
                    total += 1
                    zh_norm = _normalize_zh_text(acc)
                    if not zh_norm:
                        accs_en.append("")
                        miss += 1
                        continue

                    cached = cache.get((code_tail, zh_norm))
                    if cached is not None:
                        accs_en.append(cached)
                        continue

                    en = ""

                    # 1) exact (code + zh)
                    en = by_code_and_zh.get((code_tail, zh_norm), "")
                    if en:
                        hit_exact += 1
                        accs_en.append(en)
                        cache[(code_tail, zh_norm)] = en
                        continue

                    # 2) fuzzy (code + zh)
                    cands = zh_keys_by_code.get(code_tail) or []
                    best = _best_fuzzy_match(zh_norm, cands, threshold=0.85)
                    if best:
                        en = by_code_and_zh.get((code_tail, best), "")
                        if en:
                            hit_fuzzy_code += 1
                            accs_en.append(en)
                            cache[(code_tail, zh_norm)] = en
                            continue

                    # 3) exact global by zh
                    en = by_zh.get(zh_norm, "")
                    if en:
                        hit_exact_global += 1
                        accs_en.append(en)
                        cache[(code_tail, zh_norm)] = en
                        continue

                    miss += 1
                    accs_en.append("")
                    cache[(code_tail, zh_norm)] = ""

                    if total % 5000 == 0:
                        print(f"ç¿»è¯‘è¿›åº¦: å·²å¤„ç†é…ä»¶ {total}")

                item["accessories_en"] = accs_en

            print("\n--- ç¿»è¯‘åŒ¹é…ç»Ÿè®¡ï¼ˆtranslate_accessories.xlsxï¼‰---")
            print(f"ç¿»è¯‘è¡¨è·¯å¾„: {translate_xlsx_path}")
            print(f"æ€»é…ä»¶æ¡ç›®: {total}")
            print(f"å‘½ä¸­: ç¼–ç +åç§°ç²¾ç¡® {hit_exact}")
            print(f"å‘½ä¸­: ç¼–ç +åç§°æ¨¡ç³Š {hit_fuzzy_code}")
            print(f"å‘½ä¸­: åç§°ç²¾ç¡® {hit_exact_global}")
            print(f"æœªå‘½ä¸­: {miss}")
            print("-------------------------------------------")

        with open(host_output_path, "w", encoding="utf-8") as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
    except Exception as e:
        raise RuntimeError(f"å†™å…¥ JSON å¤±è´¥: {e}") from e

    print(f"\nâœ… JSON æ–‡ä»¶å·²ä¿å­˜åˆ°: {os.path.abspath(host_output_path)}")
    print("TASK_COMPLETE")
    return items

    # 3) ç»„è£…ç³»ç»Ÿæç¤º
    system_prompt = SYSTEM_PROMPT_TEMPLATE

    # åˆå§‹åŒ–å¯¹è¯å†å²
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "è¯·å¼€å§‹åˆ†æã€‚é¦–å…ˆåŠ è½½ `file_path` æŒ‡å‘çš„ excelï¼Œå¹¶æ‰“å° df.head() å’Œ df.columnsã€‚"}
    ]

    # 4) ä»£ç†å¾ªç¯ï¼ˆæœ¬åœ°é€šè¿‡ uv çš„éš”ç¦»ç¯å¢ƒæ‰§è¡Œï¼‰
    max_rounds = 20
    rounds = 0

    while True:
        rounds += 1
        if rounds > max_rounds:
            print("é”™è¯¯ï¼šå·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆ20æ¬¡ï¼‰ã€‚")
            break

        print("\n" + "=" * 50)
        print("ğŸ¤– [Qwen æ€è€ƒä¸­...]")
        try:
            response = litellm.completion(
                model=Qwen_MODEL,
                messages=messages,
                api_key=Qwen_API_KEY,
                api_base=Qwen_URL_BASE,
            )
        except Exception as e:
            print(f"è°ƒç”¨ LiteLLM/Qwen æ—¶å‡ºé”™: {e}")
            break

        code_to_run = response.choices[0].message.content.strip()
        messages.append({"role": "assistant", "content": code_to_run})

        print(f"ğŸ [Qwen æè®®çš„ä»£ç ]:\n{code_to_run}")
        print("=" * 50)

        # å°† LLM ä»£ç å†™å…¥ä¸´æ—¶è„šæœ¬ï¼›åœ¨é¡¶éƒ¨æ³¨å…¥è·¯å¾„å˜é‡
        tmp_script_path = None
        try:
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as tmp_file:
                header = (
                    f"file_path = r\"{input_file_path}\"\n"
                    f"output_file_path = r\"{host_output_path}\"\n"
                )
                tmp_file.write(header)
                tmp_file.write("\n\n")
                tmp_file.write(code_to_run)
                tmp_script_path = tmp_file.name
        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼šæ— æ³•å†™å…¥ä¸´æ—¶è„šæœ¬: {e}")
            break

        # ä½¿ç”¨ uv è¿è¡Œä¸´æ—¶è„šæœ¬ï¼Œæä¾›éš”ç¦»ç¯å¢ƒå¹¶å®‰è£…æ‰€éœ€ä¾èµ–
        try:
            print(f"ğŸ§ª [uv æ­£åœ¨æ‰§è¡Œ: python {tmp_script_path}]")
            proc = subprocess.run(
                [
                    "uv",
                    "run",
                    "--no-project",
                    "--with",
                    "pandas",
                    "--with",
                    "openpyxl",
                    "python",
                    tmp_script_path,
                ],
                capture_output=True,
                text=True,
            )
            output = (proc.stdout or "") + ("\n" + proc.stderr if proc.stderr else "")
            print(f"ğŸ–¥ï¸ [æœ¬åœ° uv è¾“å‡º]:\n{output}")
        except FileNotFoundError:
            print("é”™è¯¯ï¼šæœªæ‰¾åˆ° `uv` å¯æ‰§è¡Œæ–‡ä»¶ã€‚è¯·å…ˆå®‰è£… uvï¼šhttps://docs.astral.sh/uv/ å¹¶ç¡®ä¿åœ¨ PATH ä¸­ã€‚")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if tmp_script_path and os.path.exists(tmp_script_path):
                os.unlink(tmp_script_path)
            break
        except Exception as e:
            print(f"è¿è¡Œ uv æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if tmp_script_path and os.path.exists(tmp_script_path):
                os.unlink(tmp_script_path)
            break
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if tmp_script_path and os.path.exists(tmp_script_path):
                try:
                    os.unlink(tmp_script_path)
                except Exception:
                    pass

        # ä»»åŠ¡å®Œæˆåˆ¤æ–­
        if "TASK_COMPLETE" in output:
            print("\nâœ… ä»»åŠ¡å®Œæˆä¿¡å·å·²æ”¶åˆ°ï¼")
            if os.path.exists(host_output_path):
                print(f"âœ… æˆåŠŸåˆ›å»ºè¾“å‡ºæ–‡ä»¶: '{host_output_path}'")
                # è¯»å– JSON æ–‡ä»¶å†…å®¹å¹¶è¿”å›
                try:
                    with open(host_output_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    return json_data
                except FileNotFoundError:
                    print(f"âš ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç”Ÿæˆçš„ JSON æ–‡ä»¶ '{host_output_path}'")
                    raise
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ é”™è¯¯ï¼šJSON æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                    raise
            else:
                print("âš ï¸ è­¦å‘Šï¼šæ”¶åˆ° TASK_COMPLETE ä¿¡å·ï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ã€‚")
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ç”Ÿæˆçš„ JSON æ–‡ä»¶: {host_output_path}")

        # å°†æ‰§è¡Œè¾“å‡ºåé¦ˆç»™ LLM ç»§ç»­è¿­ä»£
        messages.append({"role": "user", "content": f"ä»£ç å·²æ‰§è¡Œã€‚è¾“å‡ºï¼š\n{output}"})
    
    # å¦‚æœå¾ªç¯ç»“æŸä½†æœªå®Œæˆä»»åŠ¡ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise RuntimeError("ä»»åŠ¡æœªå®Œæˆï¼šå·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°æˆ–å‘ç”Ÿé”™è¯¯")

if __name__ == "__main__":
    """ä¸»å‡½æ•°ï¼Œç”¨äºå‘½ä»¤è¡Œç›´æ¥è¿è¡Œ"""
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="backend/data_test/ref_BOM_products.xlsx")
    parser.add_argument("--output", default="backend/data_test/ref_BOM_products.json")
    args = parser.parse_args()

    extract_node_name_from_xlsx(args.input, output_file_path=args.output)